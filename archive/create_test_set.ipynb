{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test set\n",
    "\n",
    "In the file `explore_data.ipynb`, I found that a token cutoff of 3000 had over 95% of all tokens and articles. I also found that a cutoff of 30% symbol tokens captures about 90% of tokens and articles. In order to have a test set that is not to onerous to produce or too poor quality to recover I am going to combine both of these metrics\n",
    "\n",
    "Need to do\n",
    "- Load all articles, keep only the number of tokens, the page and the number of symbols\n",
    "- sum to page level\n",
    "- calc fract symbols\n",
    "- review distribution\n",
    "- minimum token count, maximum symbol fract\n",
    "- subset pages\n",
    "- sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time  # Make sure to import the time module\n",
    "import shutil\n",
    "import PyPDF2\n",
    "\n",
    "from helper_functions import identify_file, find_pdf_path, extract_pages_from_pdf, process_pdfs\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "directory = './data/ncse_text_chunks'\n",
    "\n",
    "dev_transcripts = 'data/dev_data_transcript'\n",
    "\n",
    "dev_gpt4_results = 'data/dev_data_gpt-4-turbo-preview'\n",
    "dev_gpt3_results = 'data/dev_data_gpt-3.5-turbo'\n",
    "\n",
    "image_path = os.getenv(\"image_path\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the conversion from the publication ID to the folder path of the archived images from the NCSE figshare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder2id_df = pd.DataFrame([{'folder':'English_Womans_Journal_issue_PDF_files', 'publication_id':24},\n",
    " {'folder':'Leader_issue_PDF_files/Leader_issue_PDF_files', 'publication_id':20},\n",
    " {'folder':'Monthly_Repository_issue_PDF_files', 'publication_id':22},\n",
    " {'folder':'Northern_Star_issue_PDF_files', 'publication_id':27},\n",
    " {'folder':'Publishers_Circular_issue_PDF_files', 'publication_id':26},\n",
    " {'folder':'Tomahawk_issue_PDF_files/Tomahawk_issue_PDF_files', 'publication_id':19}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_list = pd.read_parquet('data/periodicals_page.parquet')\n",
    "\n",
    "periodicals_issue = pd.read_parquet('data/periodicals_issue.parquet')\n",
    "\n",
    "periodicals_publication = pd.read_parquet('data/periodicals_publication.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_info = page_list.rename(columns = {'number':'page_number'}).merge(\n",
    "    periodicals_issue[['id', 'publication_id', 'issue_date']].rename(columns = {'id':'issue_id'}), on = 'issue_id'\n",
    ").merge(\n",
    "   periodicals_publication[['id', 'slug', 'title']].rename(columns = {'id':'publication_id'}), on='publication_id' \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1842)\n",
    "pages_for_transcription_1 = page_info.groupby('title').sample(10).merge(folder2id_df, on = 'publication_id')\n",
    "\n",
    "pages_for_transcription_1['pdf_path'] =pages_for_transcription_1.apply(lambda row: find_pdf_path(image_path, row['folder'],  row['issue_date'].strftime('%Y-%m-%d')), axis=1)\n",
    "pages_for_transcription_1['file_name'] = pages_for_transcription_1['pdf_path'].apply(os.path.basename)\n",
    "\n",
    "\n",
    "process_pdfs(pages_for_transcription_1, output_folder=\"data/pdfs_for_transcription\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
