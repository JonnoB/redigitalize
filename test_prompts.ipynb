{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #all functions need to be pre-pended with 'pd.' e.g. the DataFrame function must be written as 'pd.DataFrame'\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import time\n",
    "import os \n",
    "\n",
    "\n",
    "from helper_file import chunk_text_with_overlap, RateLimiter, get_model_response, find_overlap, merge_chunks\n",
    "\n",
    "enc = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "\n",
    "load_path = './data/northern_star_raw'\n",
    "save_path = './data/northern_star_recovered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'northern_star_1837-12-02_ed_1_1_p_1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(load_path,file_name), 'r') as file:\n",
    "    # Read the file and split into a list at each line break\n",
    "    lines = file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_running_total(df, column, threshold):\n",
    "    \"\"\"\n",
    "    Groups rows of a DataFrame based on a running total of a specified column.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame.\n",
    "    - column: The column name (string) on which the running total is calculated.\n",
    "    - threshold: The value at which the group should change.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with an additional column 'group' indicating the group number.\n",
    "    \"\"\"\n",
    "    running_total = 0\n",
    "    group_number = 0\n",
    "    groups = []  # List to hold the group numbers\n",
    "    \n",
    "    for value in df[column]:\n",
    "        if running_total + value > threshold:\n",
    "            running_total = value  # Reset running total for the new group\n",
    "            group_number += 1  # Increment group number\n",
    "        else:\n",
    "            running_total += value  # Add to the running total\n",
    "        \n",
    "        groups.append(group_number)\n",
    "    \n",
    "    df['group'] = groups\n",
    "    return df\n",
    "\n",
    "\n",
    "import tiktoken  # Ensure tiktoken library is installed\n",
    "\n",
    "def split_strings_by_tokens(strings, max_tokens):\n",
    "    \"\"\"\n",
    "    Splits strings in the list that exceed max_tokens into two parts and replaces\n",
    "    the original string with these two parts, based on token count.\n",
    "\n",
    "    Parameters:\n",
    "    - strings: List of strings to be processed.\n",
    "    - max_tokens: Maximum number of tokens allowed before splitting a string.\n",
    "\n",
    "    Returns:\n",
    "    - A new list of strings where strings longer than max_tokens are split into two.\n",
    "    \"\"\"\n",
    "    result = []  # Initialize the result list\n",
    "    enc = tiktoken.encoding_for_model('gpt-3.5-turbo')  # Encoding for token counting\n",
    "    \n",
    "    for string in strings:\n",
    "        token_count = len(enc.encode(string))\n",
    "        if token_count > max_tokens:\n",
    "            # Split the string into words for more granular control\n",
    "            words = string.split()\n",
    "            # Attempt to split the string into two parts, near the middle, but adjusted for token count\n",
    "            for split_index in range(len(words) // 2, len(words)):\n",
    "                first_half = ' '.join(words[:split_index])\n",
    "                second_half = ' '.join(words[split_index:])\n",
    "                \n",
    "                # Check if the split results in both halves being under the max_tokens limit\n",
    "                if len(enc.encode(first_half)) <= max_tokens and len(enc.encode(second_half)) <= max_tokens:\n",
    "                    result.extend([first_half, second_half])\n",
    "                    break\n",
    "        else:\n",
    "            result.append(string)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split list up to make it able to be passed to gpt\n",
    "lines = split_strings_by_tokens(lines, 3000)\n",
    "\n",
    "token_data = pd.DataFrame({'tokens':[len(enc.encode(line)) for line in lines]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = group_by_running_total(token_data, 'tokens', 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "group\n",
       "0      391\n",
       "1      450\n",
       "2      526\n",
       "3     1054\n",
       "4     1162\n",
       "5     1123\n",
       "6      269\n",
       "7     1246\n",
       "8     1198\n",
       "9      876\n",
       "10    1171\n",
       "11     945\n",
       "Name: tokens, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.groupby('group')['tokens'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "group\n",
       "0     16\n",
       "1     27\n",
       "2     37\n",
       "3     43\n",
       "4     44\n",
       "5     50\n",
       "6     59\n",
       "7     60\n",
       "8     63\n",
       "9     66\n",
       "10    69\n",
       "11    71\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.groupby('group').apply(lambda x: x.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = '\\n'.join(lines[0:17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = f\"\"\"The below text is from \"The Northern Star\" a newspaper based in Leeds UK. the edition is from 2nd December 1837. \n",
    "The text cover may multiple articles and adverts. Each piece within the newspaper page is separated by at least one line, titles are on their own line, \n",
    "adverts begin \"AD\" with a number after with no space after the letters. Please recover the OCR and format the text appropriately':::'\n",
    "::: {chunk} :::\n",
    "\"\"\"\n",
    "\n",
    "rate_limiter = RateLimiter(50000)\n",
    "\n",
    "response = get_model_response(prompt_text, 'You are an expert in recovery of poor quality OCR.', \n",
    "                                rate_limiter, engine=\"gpt-3.5-turbo\").choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(os.path.join(save_path, file_name), 'w') as file:\n",
    "    # Write the text string to the file\n",
    "    file.write(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text chunk: 0/12 complete, time taken 40.17 seconds\n",
      "text chunk: 1/12 complete, time taken 69.82 seconds\n",
      "text chunk: 2/12 complete, time taken 24.98 seconds\n",
      "text chunk: 3/12 complete, time taken 36.21 seconds\n",
      "text chunk: 4/12 complete, time taken 31.09 seconds\n",
      "text chunk: 5/12 complete, time taken 61.16 seconds\n",
      "text chunk: 6/12 complete, time taken 27.21 seconds\n",
      "text chunk: 7/12 complete, time taken 54.83 seconds\n",
      "text chunk: 8/12 complete, time taken 37.89 seconds\n",
      "text chunk: 9/12 complete, time taken 21.54 seconds\n",
      "text chunk: 10/12 complete, time taken 36.34 seconds\n",
      "text chunk: 11/12 complete, time taken 29.34 seconds\n"
     ]
    }
   ],
   "source": [
    "response_list = []\n",
    "\n",
    "rate_limiter = RateLimiter(50000)\n",
    "\n",
    "chunk_num = 0\n",
    "\n",
    "group_list = grouped_df.groupby('group').apply(lambda x: x.index[-1]).to_list()\n",
    "\n",
    "group_start = 0\n",
    "\n",
    "for group in group_list:\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    group_end = group + 1\n",
    "    chunk = '\\n'.join(lines[group_start:group_end])\n",
    "\n",
    "    prompt_text = f\"\"\"The below text is from \"The Northern Star\" a newspaper based in Leeds UK. the edition is from 2nd December 1837. \n",
    "The text cover may multiple articles and adverts. Each piece within the newspaper page is separated by at least one line, titles are on their own line, \n",
    "adverts begin \"AD\" with a number after with no space after the letters. Please recover the OCR and format the text appropriately':::'\n",
    "::: {chunk} :::\n",
    "\"\"\"\n",
    "\n",
    "    response = get_model_response(prompt_text, 'You are an expert in recovery of poor quality OCR.', \n",
    "                                rate_limiter, engine=\"gpt-4-0125-preview\").choices[0].message.content\n",
    "    \n",
    "    response_list.append(response)\n",
    "\n",
    "    end_time = time.time()  # End timing\n",
    "    chunk_time = end_time - start_time  # Calculate the time taken for this chunk\n",
    "\n",
    "    group_start = group_end\n",
    "\n",
    "    print(f\"text chunk: {chunk_num}/{len(group_list)} complete, time taken {chunk_time:.2f} seconds\")\n",
    "    chunk_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(os.path.join(save_path, 'page_1_gpt4.txt'), 'w') as file:\n",
    "    # Write the text string to the file\n",
    "    file.write(' '.join(response_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
